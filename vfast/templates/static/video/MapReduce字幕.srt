1
00:00:00,000 --> 00:00:05,480
Hadoop的工作流程看似很简单，首先把数据写入HDFS

2
00:00:05,480 --> 00:00:08,980
然后进行计算，最后输出结果就可以了。

3
00:00:08,980 --> 00:00:13,000
但是在获取结果的道路上却是步步维艰

4
00:00:13,000 --> 00:00:16,480
因为需要海量数据能够快速的计算

5
00:00:16,480 --> 00:00:21,020
我们需要细化每一步操作才能得到分析后的数据

6
00:00:21,020 --> 00:00:24,500
MapReduce作为Hadoop中最重要的组件之一

7
00:00:24,500 --> 00:00:26,980
但是承担着分析数据的工作

8
00:00:26,980 --> 00:00:30,480
而且还能够快速的完成海量数据的计算

9
00:00:30,480 --> 00:00:34,020
MapReduce工作需要和hdfs进行交互

10
00:00:34,020 --> 00:00:36,400
通过分布式并行技术处理数据

11
00:00:36,400 --> 00:00:39,380
但是它比较倾向于对数据的计算

12
00:00:39,380 --> 00:00:41,380
想要学习MapReduce

13
00:00:41,380 --> 00:00:44,300
其实就是对MapReduce的工作流程的学习

14
00:00:44,300 --> 00:00:46,720
我们需要知道它执行的过程

15
00:00:46,820 --> 00:00:50,000
才能够在过程中去进行设置修改

16
00:00:50,000 --> 00:00:52,520
下面我为大家绘制一条路线

17
00:00:52,520 --> 00:00:57,080
通过这条路线来详细的了解一下MapReduce的工作过程

18
00:00:57,080 --> 00:01:01,040
MapReduce工作过程其实是Map和Reduce两个阶段

19
00:01:01,040 --> 00:01:03,500
比如现在有很多英文文档

20
00:01:03,500 --> 00:01:07,360
需要对文档中每个单词出现的次数进行统计

21
00:01:07,360 --> 00:01:09,560
我们称为词频统计

22
00:01:09,560 --> 00:01:12,060
那么MapReduce该怎么工作呢？

23
00:01:12,060 --> 00:01:15,520
首先要对这个文件进行拆分

24
00:01:15,520 --> 00:01:17,520
拆分成很多个分片

25
00:01:17,520 --> 00:01:20,420
每一个分片就对应着一个Map任务

26
00:01:20,420 --> 00:01:23,360
所以这里会产生很多个Map任务

27
00:01:23,360 --> 00:01:26,800
Map任务的主要工作就是将文档中

28
00:01:26,800 --> 00:01:29,460
分片里的每一个单词拆分出来

29
00:01:29,460 --> 00:01:31,380
并标记为1

30
00:01:31,380 --> 00:01:33,380
形成键值对的形式

31
00:01:33,380 --> 00:01:36,260
key就是单词value就是1

32
00:01:36,260 --> 00:01:39,760
每一个Map的结果都会放到内存中

33
00:01:39,760 --> 00:01:42,920
当所有的Map都执行完毕后

34
00:01:42,920 --> 00:01:45,160
内存会有一个洗牌的过程

35
00:01:45,160 --> 00:01:48,730
会将key相同的键值对放到一个组里去

36
00:01:48,730 --> 00:01:52,200
有多少个不同的单词就会有多少个组

37
00:01:52,200 --> 00:01:55,510
这些组会按照key的升序排序

38
00:01:55,510 --> 00:01:58,200
这个过程呢我们叫做shuffle

39
00:01:58,200 --> 00:02:00,530
当洗牌完毕后

40
00:02:00,530 --> 00:02:03,040
就可以进入我们的Reduce了

41
00:02:03,040 --> 00:02:07,530
Reduce阶段就是将key相同的所有value值进行累加

42
00:02:07,530 --> 00:02:10,370
因为我们给每一个单词的拆分的时候

43
00:02:10,370 --> 00:02:12,370
都已经标记为 1

44
00:02:12,370 --> 00:02:17,860
所以累加的结果就是key这个单词在文档中出现的次数

45
00:02:17,860 --> 00:02:20,440
每组都会进行这样的处理

46
00:02:20,440 --> 00:02:25,330
最后同样形成键值对的形式输出成一个文件

47
00:02:25,330 --> 00:02:31,260
这样就形成了对这个文件中的词频的统计

